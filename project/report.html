<!DOCTYPE html>
<html>
<head>
  <title>KPL Project: Depth of Field</title>
</head>
<body>
<div>
  <h1>Modelling Depth of Field in a WGSL Path-Tracing Shader</h1>
</div>
<div>
  <h2>Introduction</h2>
  <p>
    Our ray tracing has so far imitated a pinhole camera with infinite depth of field.
    In a pinhole camera, the image plane (holding either the piece of film or digital sensor)
    sits behind a very small aperture. Only light from the scene that passes through the pinhole 
    may create an image on the image plane. With an ideal pinhole camera, the hole would 
    be infinitesimally small so that any point on the image can only be 
    illuminated by light from a single ray (the ray defined by the point on the image plane and the point of the pinhole).
  </p>
  <p>
    This ideal pinhole model is attractive for ray tracing. Because only one ray can illuminate a given point,
    we can trivially sample points within a pixel on the image sensor, find the ray that can illuminate that point, 
    and then trace that single ray into the scene to find how it would be illuminated by interacting with objects and
    light sources. This gives a realistic rendering that approximates a physical camera and scene.
    However, in physical cameras an infinitesimal pinhole is not achievable and in fact 
    many camera systems use very large apertures to allow more light to hit the image plane.
    In a physical camera, the image is capture by essentially integrating (either chemically or electronically)
    the light received on the image plane across the time interval when the shutter is open. 
    A camera must be held steady while its shutter is open to prevent smearing the image across the image 
    plane producing a blurry image. Therefore, a shorter, or faster, shutter time is desirable.
  </p>
  <p>
    To achieve fast shutter speeds the aperture of a camera can be opened wider to increase the amount 
    of light received from the scene. However, as the aperture widens light from many different angles can hit the same 
    point on the image plane. Without some way to focus the light, the resulting image captured will be a blurred mess.
    This is the role of camera optics. Specifically designed curved translucent materials can cause light to refract at different 
    angles to focus to a point even with a wide aperture. However, an optical system can only focus light 
    to a point if it is incident on the optical lens at a particular angle. Light rays from infinitely far objects 
    will arrive at the lens essentially parallel, while those from closer and closer objects will arrive 
    at a greater and greater angle relative to each other. Because the lens' refraction depends on the incident angle,
    an optical system can only focus light from objects some fixed distance from the lens to a point. The distance that is in focus
    can usually be adjusted by moving the optics relative to the image plane. However, this means that only objects that distance from the camera 
    will be sharp in the captured image. For objects closer or further, their light will not come to a point on the image plane 
    but will instead end up somewhere in a circle on the image plane. This is called the circle of confusion, and the larger it is for 
    a given object the blurrier that object will appear in the image.
  </p>
  <p>
    In photography, it is therefore common for photos to include some sections in focus and others out of focus. This is 
    a physical effect that people are accustomed to seeing. It is even used for an artistic effect called bokeh, common in 
    portrait photography (because the subject will be close to the camera and thus the background often very out of focus).
    For a realistic ray-traced render it makes sense to implement a model of the wide-aperture optics in physical cameras.
  </p>
  <p>
    To be exact, the circle of confusion is due to a camera collecting light not at a single point but across a larger aperture.
    While the light incident on an aperture from some distance away can be focused to a point, for other distances it will be possible 
    for light rays from the same point in the scene to arrive at different places on the aperture and then be bent by the optics
    to land on different places on the image plane. Because all light must pass through the aperture, the blurred result 
    is only a circle if the aperture is a circle. More generally, whatever shape the aperture is will result in defocused light 
    arriving in a pattern of that shape on the image sensor. This is sometimes used artistically by photographers with interesting aperture shapes.
  </p>
  <p>
    In addition to depending on the shape of the aperture, the blur also depends on the size. At the limit, an infinitesimal aperture 
    is a pinhole as discussed above and will cause no blur for any objects. Wider apertures cause larger circles of confusion and thus
    more blur for nonfocused objects. For any aperture one plane at the right distance from the image plane will be perfectly in focus.
    Because the image plane is sampled with discrete pixels which each have some dimension, when the circle of confusion is smaller than 
    the size of a pixel, no blurriness is visible in the resulting image. Therefore, there is some range of distances around the perfectly 
    in focus plane that also appear in focus in the image. The depth of this in focus region is called the depth of field. 
    Pinhole cameras have infinite depth of field, meaning any distance is perfectly in focus. Larger apertures shrink the depth of field.
  </p>
  <p>
    The simplest optical system to model is a thin lens, where the lens is taken to essentially just shape light 
    ideally without being concerned with the intricacies of multiple refractions inside the glass. Often physical lenses 
    use multiple refractive components to reduce effects such as chromatic abberation (the separation of colors due to the dependence
    of refraction on light's wavelength). However, as our ray tracer does not model different color refraction anyway a simple ideal 
    thin lens model is sufficient.
  </p>
</div>
<div>
  <h2>Method</h2>
  <p>
    Because many rays from the scene can hit a point on the image plane, many samples are needed for each pixel to 
    show the physical effects of the wide aperture. 
  </p>
</div>
<div>
  <h2>Implementation</h2>
  <p></p>
</div>
<div>
  <h2>Results</h2>
  <p></p>
</div>
<div>
  <h2>Discussion</h2>
  <p>
    For a more realistic lens model, it would be interesting to trace paths separately for red, green, and blue light. This would 
    allow for effects such as chromatic abberation to be modeled. However, because professional lenses are usually carefully designed to 
    minimize these types of distortion, the current model does a fine job of imitating professional photography. Chromatic abberation 
    or other lens distortions common on cheaper optics might give the render a more real, gritty feel as if taken by an amateur photographer
    with cheap equipment.
  </p>
</div>
</body>
</html>